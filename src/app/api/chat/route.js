/**
 * API Route para el AI Chat
 * 
 * Endpoint: POST /api/chat
 * 
 * Maneja las conversaciones con el asistente AI usando Vercel AI SDK.
 * Conecta el AI Chat con los servicios de dominio mediante tools.
 */

import { openai } from '@ai-sdk/openai';
import { streamText, tool, convertToModelMessages, stepCountIs } from 'ai';
import { getServerSession } from 'next-auth';
import { authOptions } from '@/app/api/auth/[...nextauth]/route';
import { allTools } from '@/lib/ai/tools';
import { SYSTEM_PROMPT } from '@/lib/ai/config';

export const runtime = 'nodejs';
export const maxDuration = 30; // 30 segundos m√°ximo

/**
 * POST /api/chat
 * 
 * Maneja las peticiones del chat AI
 */
export async function POST(req) {
  try {
    // Verificar autenticaci√≥n
    // ‚úÖ CORREGIDO: Obtener sesi√≥n del servidor para pasar el token a las tools
    const session = await getServerSession(authOptions);
    
    if (!session?.user) {
      return new Response(
        JSON.stringify({ error: 'No autorizado. Debes iniciar sesi√≥n para usar el chat.' }),
        { 
          status: 401,
          headers: { 'Content-Type': 'application/json' }
        }
      );
    }

    // ‚úÖ CR√çTICO: Obtener el token de la sesi√≥n del servidor para pasarlo a las tools
    const serverToken = session.user.accessToken;
    if (!serverToken) {
      console.warn('[CHAT API] ‚ö†Ô∏è Sesi√≥n encontrada pero sin accessToken');
      return new Response(
        JSON.stringify({ error: 'No autorizado. Token de acceso no encontrado.' }),
        { 
          status: 401,
          headers: { 'Content-Type': 'application/json' }
        }
      );
    }

    // ‚úÖ CR√çTICO: Configurar el token del servidor en el contexto global
    // Esto permite que getAuthToken() lo use cuando las tools se ejecuten
    const authTokenModule = await import('@/lib/auth/getAuthToken');
    authTokenModule.setServerTokenContext(serverToken);
    
    // ‚úÖ DEBUG: Verificar que el token se configur√≥ correctamente
    console.log('[CHAT API] üîê Token configurado en contexto, verificando...');

    // Parsear mensajes del request
    const { messages } = await req.json();

    console.log('[CHAT API] üì• Mensajes recibidos del cliente:', JSON.stringify(messages, null, 2));

    if (!messages || !Array.isArray(messages)) {
      console.error('[CHAT API] ‚ùå Mensajes inv√°lidos:', { messages, isArray: Array.isArray(messages) });
      return new Response(
        JSON.stringify({ error: 'Se requieren mensajes v√°lidos' }),
        { 
          status: 400,
          headers: { 'Content-Type': 'application/json' }
        }
      );
    }

    // ‚úÖ CORRECCI√ìN DEFINITIVA: Convertir UIMessage[] a ModelMessage[] para mantener el historial completo
    // useChat env√≠a UIMessage[] con formato { id, role, parts: [{ type: 'text', text: ... }] }
    // streamText espera ModelMessage[] (formato compatible con el protocolo del SDK)
    // convertToModelMessages mantiene TODO el historial del chat (memoria del AI)
    // ‚ö†Ô∏è CR√çTICO: convertToModelMessages es ASYNC, debemos usar await
    let modelMessages;
    try {
      console.log('[CHAT API] üîÑ Convirtiendo mensajes. Cantidad:', messages.length);
      
      // ‚úÖ CR√çTICO: convertToModelMessages es async, usar await
      modelMessages = await convertToModelMessages(messages);
      
      console.log('[CHAT API] ‚úÖ Mensajes convertidos. Tipo:', typeof modelMessages, '¬øEs Array?:', Array.isArray(modelMessages));
      console.log('[CHAT API] ‚úÖ Cantidad de mensajes convertidos:', modelMessages?.length);
      
      if (!Array.isArray(modelMessages)) {
        console.error('[CHAT API] ‚ùå convertToModelMessages no devolvi√≥ un array:', {
          tipo: typeof modelMessages,
          valor: modelMessages,
          esPromise: modelMessages instanceof Promise,
        });
        throw new Error(`convertToModelMessages no devolvi√≥ un array. Tipo: ${typeof modelMessages}`);
      }
      
      console.log('[CHAT API] ‚úÖ Primer mensaje convertido:', JSON.stringify(modelMessages[0], null, 2));
    } catch (conversionError) {
      console.error('[CHAT API] ‚ùå Error al convertir mensajes:', conversionError);
      console.error('[CHAT API] ‚ùå Stack:', conversionError.stack);
      return new Response(
        JSON.stringify({ 
          error: 'Error al convertir mensajes',
          details: conversionError.message 
        }),
        { 
          status: 500,
          headers: { 'Content-Type': 'application/json' }
        }
      );
    }

    // Convertir tools a formato del AI SDK
    console.log('[CHAT API] üîß Registrando tools. Total de tools:', Object.keys(allTools).length);
    const tools = {};
    for (const [name, toolDef] of Object.entries(allTools)) {
      console.log(`[CHAT API] üîß Registrando tool: ${name}`, {
        hasDescription: !!toolDef.description,
        hasParameters: !!toolDef.parameters,
        hasExecute: typeof toolDef.execute === 'function',
        parametersType: toolDef.parameters?._def?.typeName,
      });

      // ‚úÖ VALIDAR que parameters existe y es un ZodObject v√°lido
      if (!toolDef.parameters || typeof toolDef.parameters.parse !== 'function') {
        console.error(`[CHAT API] ‚ùå Tool ${name} has invalid parameters schema`, {
          hasParameters: !!toolDef.parameters,
          typeName: toolDef.parameters?._def?.typeName,
          parametersValue: toolDef.parameters,
        });
        throw new Error(`Tool ${name} has invalid parameters schema`);
      }

      // ‚úÖ Verificar que es un ZodObject (Zod v3)
      if (toolDef.parameters._def?.typeName !== 'ZodObject') {
        console.error(`[CHAT API] ‚ùå Tool ${name} parameters is not a ZodObject`, {
          typeName: toolDef.parameters._def?.typeName,
          _def: toolDef.parameters._def,
        });
        throw new Error(`Tool ${name} parameters must be a ZodObject, got ${toolDef.parameters._def?.typeName}`);
      }

      try {
        // ‚úÖ CORRECCI√ìN seg√∫n documentaci√≥n oficial: usar inputSchema en lugar de parameters
        // La documentaci√≥n oficial (https://ai-sdk.dev/providers/ai-sdk-providers/openai) muestra
        // que se debe usar inputSchema para describir los campos esperados
        // ‚úÖ CR√çTICO: Usar closure para capturar serverToken y authTokenModule
        // Esto asegura que el token est√© disponible cuando la tool se ejecute, incluso si el contexto global se pierde
        tools[name] = tool({
          description: toolDef.description,
          inputSchema: toolDef.parameters, // ‚úÖ Usar inputSchema (seg√∫n documentaci√≥n oficial)
          execute: async (params) => {
            // ‚úÖ CR√çTICO: Re-configurar el token en el contexto ANTES de ejecutar la tool
            // Esto garantiza que est√© disponible incluso si el contexto global se perdi√≥
            // El closure captura serverToken y authTokenModule del scope externo
            authTokenModule.setServerTokenContext(serverToken);
            
            try {
              // Ejecutar la tool con manejo de errores
              const result = await toolDef.execute(params);
              console.log(`[CHAT API] ‚úÖ Tool ${name} completada. Resultado:`, JSON.stringify(result, null, 2).substring(0, 500));
              // ‚úÖ IMPORTANTE: Las tools deben devolver JSON puro, no strings
              // El SDK convierte autom√°ticamente a JSON para Responses API (GPT-5)
              return result;
            } catch (error) {
              console.error(`[CHAT API] ‚ùå Error executing tool ${name}:`, error);
              console.error(`[CHAT API] ‚ùå Stack:`, error.stack);
              
              // Devolver error estructurado
              return {
                success: false,
                error: true,
                message: error.message || 'Error ejecutando la herramienta',
                tool: name,
              };
            }
          },
        });
        console.log(`[CHAT API] ‚úÖ Tool ${name} registrada correctamente`);
      } catch (toolError) {
        console.error(`[CHAT API] ‚ùå Error al registrar tool ${name}:`, toolError);
        console.error(`[CHAT API] ‚ùå Stack:`, toolError.stack);
        throw toolError;
      }
    }

    console.log(`[CHAT API] ‚úÖ Total de tools registradas: ${Object.keys(tools).length}`);

    // ‚úÖ CORREGIDO: Modelo actualizado a GPT-5
    // gpt-4-turbo-preview ya NO existe en OpenAI Platform
    // Usar gpt-5-mini (recomendado) o gpt-5.2 (m√°xima calidad)
    const aiModel = process.env.AI_MODEL || 'gpt-5-mini';

    console.log('[CHAT API] ü§ñ Configurando streamText:', {
      model: aiModel,
      messagesCount: modelMessages.length,
      toolsCount: Object.keys(tools).length,
      systemPromptLength: SYSTEM_PROMPT.length,
    });

    // üîÑ FLUJO DE DOS PASOS (tool ‚Üí IA ‚Üí texto) - SEG√öN DOC OFICIAL
    // 
    // Seg√∫n la documentaci√≥n oficial (ai-sdk.dev):
    // - Por defecto, streamText es de 1 paso (tool-call ‚Üí tool-result, sin texto final)
    // - Para activar multi-step tool loop, necesitamos usar stopWhen
    // 
    // PASO 1 - Ejecuci√≥n de la tool:
    //   - El modelo decide qu√© tool ejecutar bas√°ndose en el mensaje del usuario
    //   - La tool se ejecuta y devuelve datos estructurados (JSON)
    //   - El SDK reinyecta autom√°ticamente el resultado en el stream
    //
    // PASO 2 - Generaci√≥n de texto (OBLIGATORIO):
    //   - El modelo recibe el resultado de la tool como contexto
    //   - El modelo DEBE generar un mensaje de texto explicando los resultados
    //   - Este texto es lo que el usuario ver√° en el chat
    //
    // ‚úÖ CONFIGURACI√ìN CR√çTICA (seg√∫n doc oficial):
    //   - stopWhen: stepCountIs(5) ‚Üí Activa multi-step tool loop (tool ‚Üí texto)
    //   - maxSteps: 10 ‚Üí L√≠mite m√°ximo de pasos (tool calls + generaci√≥n de texto)
    //   - sendToolResultMessages: true (ver m√°s abajo) ‚Üí Permite que el SDK env√≠e tool results
    //   - SYSTEM_PROMPT ‚Üí Instruye expl√≠citamente al modelo sobre el flujo de DOS PASOS
    //
    // ‚úÖ RESPONSABILIDADES CLARAS:
    //   - Tools: Devuelven JSON puro ({ success, data, meta })
    //   - Backend: Orquesta el flujo entre tool y modelo (autom√°tico con SDK + stopWhen)
    //   - IA: Es la √∫nica responsable de convertir datos en lenguaje natural
    let result;
    try {
      result = streamText({
        model: openai(aiModel),
        system: SYSTEM_PROMPT, // ‚úÖ Instruye expl√≠citamente sobre el flujo de DOS PASOS
        messages: modelMessages, // ‚úÖ Historial completo convertido a ModelMessage[]
        tools, // ‚úÖ Tools que devuelven datos estructurados
        maxSteps: 10, // ‚úÖ L√≠mite m√°ximo de pasos
        stopWhen: stepCountIs(5), // ‚úÖ CR√çTICO: Activa multi-step tool loop (seg√∫n doc oficial)
        // Con stopWhen: stepCountIs(5), el modelo puede:
        //   - Paso 1: Ejecutar tool
        //   - Paso 2: Recibir tool result
        //   - Paso 3: Generar texto explicando resultados
        //   - Hasta 5 pasos en total (suficiente para tool + texto)
      });
      console.log('[CHAT API] ‚úÖ streamText configurado correctamente');
    } catch (streamTextError) {
      console.error('[CHAT API] ‚ùå Error al configurar streamText:', streamTextError);
      console.error('[CHAT API] ‚ùå Stack:', streamTextError.stack);
      console.error('[CHAT API] ‚ùå Mensajes que causaron el error:', JSON.stringify(modelMessages, null, 2));
      throw streamTextError;
    }

    // Devolver stream de respuesta con flujo de DOS PASOS habilitado
    // 
    // En AI SDK v6, `toUIMessageStreamResponse()` con `sendToolResultMessages: true` garantiza:
    // 
    // 1. El SDK env√≠a el resultado de la tool como mensaje intermedio (tool result)
    // 2. El SDK permite que el modelo genere un mensaje de texto final despu√©s de recibir el tool result
    // 3. El frontend (useChat) recibe AMBOS mensajes:
    //    - El tool result (opcionalmente renderizable como indicador de "consultando...")
    //    - El mensaje de texto final del assistant (obligatorio, lo que el usuario lee)
    //
    // ‚ö†Ô∏è SIN `sendToolResultMessages: true`:
    //   - El SDK solo env√≠a el tool result
    //   - El modelo NO genera mensaje de texto final
    //   - El chat queda en silencio (respuesta vac√≠a)
    //
    // ‚úÖ CON `sendToolResultMessages: true`:
    //   - El SDK env√≠a tool result + mensaje de texto final
    //   - El usuario ve la respuesta explicada en lenguaje natural
    //   - El flujo de DOS PASOS funciona correctamente
    console.log('[CHAT API] ‚úÖ Preparando respuesta stream con flujo de DOS PASOS habilitado');
    
    // ‚úÖ Limpiar el contexto del token antes de devolver la respuesta
    authTokenModule.clearServerTokenContext();
    
    return result.toUIMessageStreamResponse({
      sendToolResultMessages: true, // ‚úÖ CR√çTICO: Habilita el flujo de DOS PASOS (tool ‚Üí texto)
      // Con esta opci√≥n, el SDK garantiza que el modelo genere un mensaje de texto
      // despu√©s de ejecutar una tool, cumpliendo el flujo: datos ‚Üí lenguaje natural
    });
  } catch (error) {
    // ‚úÖ Asegurar que limpiamos el contexto incluso si hay error
    try {
      if (authTokenModule?.clearServerTokenContext) {
        authTokenModule.clearServerTokenContext();
      }
    } catch (cleanupError) {
      console.error('[CHAT API] ‚ùå Error al limpiar contexto de token:', cleanupError);
    }
    
    console.error('[CHAT API] ‚ùå ERROR GENERAL en chat API:', error);
    console.error('[CHAT API] ‚ùå Tipo de error:', error.constructor.name);
    console.error('[CHAT API] ‚ùå Mensaje:', error.message);
    console.error('[CHAT API] ‚ùå Stack:', error.stack);
    console.error('[CHAT API] ‚ùå Error completo:', JSON.stringify(error, Object.getOwnPropertyNames(error), 2));
    
    return new Response(
      JSON.stringify({ 
        error: 'Error interno del servidor',
        message: error.message,
        type: error.constructor.name
      }),
      { 
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      }
    );
  }
}

